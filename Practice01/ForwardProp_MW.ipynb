{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ddf3fd-b2e2-4152-9237-2f7c13caa001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n请大家使用numpy库完成relu, derivation_relu, sigmoid三个函数的填空，以及forward、backward和train中部分功能的实现\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "请大家使用numpy库完成relu, derivation_relu, sigmoid三个函数的填空，以及forward、backward和train中部分功能的实现\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7bdf36c-eeea-47e0-8e22-10726737531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21c9f68-5c3b-41f3-8d64-f3961f1dc0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z: (batch_size, hidden_size)\n",
    "    return:\n",
    "        a: (batch_size, hidden_size)激活值\n",
    "    \"\"\"\n",
    "    a = np.maximum(0,z) #X和Y逐位进行比较,选择最大值\n",
    "    # pass\n",
    "    return a\n",
    "\n",
    "def derivation_relu(z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z: (batch_size, hidden_size)\n",
    "    return:\n",
    "        dz: (batch_size, hidden_size)导数值\n",
    "    \"\"\"\n",
    "    \n",
    "    dz = np.ones(z.shape)#1矩阵方法\n",
    "    dz[z<=0] = 0\n",
    "    # pass\n",
    "    return dz\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        z: (batch_size, hidden_size)\n",
    "    return:\n",
    "        a: (batch_size, hidden_size)激活值\n",
    "    \"\"\"\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    # pass\n",
    "    return a\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: (batch_size, ) 每个样本的真实label\n",
    "        y_hat: (batch_size, output_size)， 网络的输出预测得分，已经过sigmoid概率化。output_size即分类类别数\n",
    "    return:\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    n_batch = y_hat.shape[0] # 样本数量\n",
    "    loss = -np.sum(np.log(y_hat)) / n_batch # loss = -np.sum(-(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n",
    "    return loss\n",
    "\n",
    "def derivation_sigmoid_cross_entropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: (batch_size, output_size)， 网络的输出预测得分, 还没有进行 softmax概率化\n",
    "        y: (batch_size, ) 每个样本的真实label\n",
    "    \n",
    "    Return:\n",
    "        \\frac {\\partial C}{\\partial z^L}\n",
    "        (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    y_hat -= 1  #dz\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3661426-6383-45dd-a644-7bacba760ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"\n",
    "    fully-connected neural network\n",
    "    Attributions:\n",
    "        sizes: list, 输入层、隐藏层、输出层尺寸\n",
    "        num_layers: 神经网络的层数\n",
    "        weights: list, 每个元素是一层神经网络的权重\n",
    "        bias: list, 每个元素是一层神经网络的偏置\n",
    "        dws: list，存储权重梯度\n",
    "        dbs: list，存储偏置梯度\n",
    "        zs: list，存储前向传播临时变量\n",
    "        _as：list，存储前向传播临时变量\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes):\n",
    "        #sizes=[2,3,2]\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes) #3\n",
    "        #随机产生每条连线的权重\n",
    "        self.weights = [np.random.randn(i, j) for i, j in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        #随机产生隐层与输出层中每个神经元的偏置（0-1）\n",
    "        self.bias = [np.random.randn(1, j) for j in self.sizes[1:]]\n",
    "        self.dws = None\n",
    "        self.dbs = None\n",
    "        self.zs = [] \n",
    "        self._as = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        x: (batch_size, input_size)\n",
    "        \"\"\"\n",
    "        a = x\n",
    "        self._as.append(a)\n",
    "        for weight, bias in zip(self.weights[:-1], self.bias[:-1]):\n",
    "            # 计算临时变量z和a并存入self.zs和self._as\n",
    "            z = np.dot(a,weight) + bias\n",
    "            a = relu(z)\n",
    "            self.zs.append(z)\n",
    "            self._as.append(a)\n",
    "            \n",
    "            #########################################\n",
    "        logits = np.dot(a, self.weights[-1]) + self.bias[-1]\n",
    "        y_hat = sigmoid(logits)\n",
    "        self.zs.append(logits)\n",
    "        self._as.append(y_hat)\n",
    "        return y_hat \n",
    "\n",
    "    def backward(self, x, y):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        Args:\n",
    "            x: (batch_size, input_size)\n",
    "            y: (batch_size, )\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        loss = cross_entropy(y, y_hat) \n",
    "\n",
    "        ################# 反向传播梯度计算 ##############################\n",
    "        # 输出层误差\n",
    "        dl = derivation_sigmoid_cross_entropy(y, y_hat)\n",
    "        print(dl)\n",
    "        n = len(x)\n",
    "        # 最后一层的梯度\n",
    "        # 每个样本得的梯度求和、求平均\n",
    "        self.dws[-1] = np.dot(self._as[-2].T, dl) / n  \n",
    "        self.dbs[-1] = np.sum(dl, axis=0, keepdims=True) / n \n",
    "        # 计算梯度\n",
    "        for i in range(2, self.num_layers):\n",
    "            # 计算梯度并存入self.dws和self.dbs，注意矩阵乘法和逐元素乘法\n",
    "            dl = np.dot(dl, self.weights[-i+1].T) * derivation_relu(self.zs[-i])\n",
    "            self.dws[-i] = np.dot(self._as[-i-1].T, dl) / n\n",
    "            self.dbs[-i] = np.sum(dl, axis=0, keepdims=True) / n\n",
    "            ############################################################\n",
    "            \n",
    "        self.zs = [] \n",
    "        self._as = []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"清空梯度\"\"\"\n",
    "        self.dws = [np.zeros((i, j)) for i, j in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.dbs = [np.zeros((1, j)) for j in self.sizes[1:]]\n",
    "        \n",
    "    def optimize(self, learning_rate):\n",
    "        \"\"\"更新梯度\"\"\"\n",
    "        self.weights = [weight - learning_rate * dw for weight, dw in zip(self.weights, self.dws)]\n",
    "        self.bias = [bias - learning_rate * db for bias, db in zip(self.bias, self.dbs)]\n",
    "\n",
    "        \n",
    "def train():\n",
    "    \n",
    "    n_batch = 5\n",
    "    n_input_layer = 2\n",
    "    n_hidden_layer = 3\n",
    "    n_output_layer = 2\n",
    "    n_class = 2\n",
    "    x = np.random.rand(n_batch, n_input_layer) # 5行 2列\n",
    "    y = np.random.randint(0, n_class, size=n_batch) #返回一个 0 1 的5个值的array (5,1)\n",
    "    net = Network((n_input_layer, n_hidden_layer, n_output_layer))\n",
    "    print('initial weights:', net.weights)\n",
    "    print('initial bias:', net.bias)\n",
    "    # 执行梯度计算\n",
    "\n",
    "    net.forward(x)\n",
    "    net.zero_grad()\n",
    "    net.backward(x,y)\n",
    "    net.optimize(0.01)\n",
    "    #net.zero_grad()\n",
    "    \n",
    "    ##############\n",
    "    print('updated weights:', net.weights)\n",
    "    print('updated bias:', net.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1658c25f-adeb-42c0-bfbd-4c64c6cd7ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights: [array([[ 0.11065484, -1.70818053, -0.75418618],\n",
      "       [ 0.32770487, -1.68940839,  0.03553788]]), array([[-1.51670536, -1.33758874],\n",
      "       [ 2.09150799,  0.4039313 ],\n",
      "       [ 1.10866779,  1.86360581]])]\n",
      "initial bias: [array([[ 0.1136647 , -0.09211859,  0.6123297 ]]), array([[0.31869108, 1.08240231]])]\n",
      "[[-0.39810227 -0.15356417]\n",
      " [-0.51646386 -0.32229427]\n",
      " [-0.48134031 -0.26237657]\n",
      " [-0.35210271 -0.13816738]\n",
      " [-0.42282006 -0.19049196]]\n",
      "updated weights: [array([[ 0.10662929, -1.70818053, -0.75236322],\n",
      "       [ 0.32395669, -1.68940839,  0.03866206]]), array([[-1.51543716, -1.33697334],\n",
      "       [ 2.09150799,  0.4039313 ],\n",
      "       [ 1.11007977,  1.86420484]])]\n",
      "updated bias: [array([[ 0.10422556, -0.09211859,  0.61877326]]), array([[0.32303274, 1.0845361 ]])]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f32285-dcba-4721-9b30-bc81bf9d8eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46013f-a8ce-465b-b8a0-6817b4f194ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
